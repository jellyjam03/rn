{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tema 2 - Perceptron <br>\n",
    "Ne propunem sa cream un model capabil sa recunoasca imagini alb-negru cu cifre scrise de mana.<br>"
   ],
   "id": "4c89c23651f05293"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Initializarea datelor<br>\n",
    "Folosim dataset-ul MNIST pentru antrenarea si testarea modelului."
   ],
   "id": "4bf3eb5e8397a06e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T07:22:59.313679Z",
     "start_time": "2024-10-21T07:22:59.285240Z"
    }
   },
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.ma.core import reshape\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data',\n",
    "    transform=lambda x: np.array(x).flatten(),\n",
    "    download=True,\n",
    "    train=is_train)\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "    return mnist_data, mnist_labels"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T07:22:59.902338Z",
     "start_time": "2024-10-21T07:22:59.886716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_data() -> (np.array, np.array, np.array, np.array):\n",
    "    train_x, train_y = download_mnist(True)\n",
    "    test_x, test_y = download_mnist(False)\n",
    "    \n",
    "    #convertim datele in np.array s\n",
    "    train_x = np.array(train_x)\n",
    "    train_y = np.array(train_y)\n",
    "    test_x = np.array(test_x)\n",
    "    test_y = np.array(test_y)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "    "
   ],
   "id": "19d790b2e5f1af7c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Normalizarea setului de date + one-hot encoding<br>\n",
    "Valorile din seturile de date sunt numere intregi cuprinse in intervalul $[0, 256]$. Pentru a normaliza datele, vom imparti la 256, aducand valorile in intervalul $[0. 1]$. <br>\n",
    "<br>\n",
    "Q: De ce ajuta sa normalizam setul?<br>\n",
    "Cred ca mi-am raspuns singur la intrebare; numerele ajung foarte mari daca nu facem asta."
   ],
   "id": "48897dbedb0ab5ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T07:22:59.964825Z",
     "start_time": "2024-10-21T07:22:59.949203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_data(v: np.array) -> np.array:\n",
    "    return v / 256"
   ],
   "id": "cdd05b83783c4989",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T07:23:00.011688Z",
     "start_time": "2024-10-21T07:22:59.996066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def one_hot_encode(v: np.array, nr_classes: int) -> np.array:\n",
    "    return np.array([np.array([int(i == label) for i in range(nr_classes)]) for label in v])\n",
    "# test_Y = np.array([np.array([int(i == label) for i in range(10)]) for label in test_Y]) #sa nu mai uit niciodata sa fac encoding si la test labels"
   ],
   "id": "bf5802fc2ae1ca62",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Forward propagation<br>\n",
    "Utilizam functia softmax pentru a crea un output balansat. Deoarece functia foloseste exponentiere, diferentele dintre valorile claselor dupa inmultirea cu ponderile vor avea un impact mult mai mare asupra probabilitatii rezultate.<br>\n",
    "De asemenea asigura proprietatea de aditivitate numarabila a outputului (aka suma probabilitatilor este 1).\n"
   ],
   "id": "d49b3b1897486c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T07:23:24.458957Z",
     "start_time": "2024-10-21T07:23:24.433997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "def softmax(z: np.array) -> np.array:\n",
    "    exp_sum = sum([math.e**element for element in z])\n",
    "    return np.array([math.e**element / exp_sum for element in z])\n",
    "\n",
    "def make_prediction(weights: np.array, bias: np.array, instance: np.array) -> np.array:\n",
    "    #z = np.dot(weights, instance) + bias\n",
    "    z = [np.dot(weights[i], instance) + bias[i] for i in range(len(weights))]\n",
    "    return softmax(z)\n",
    "    "
   ],
   "id": "211fe217d3d8162b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Antrenarea intr-o epoca\n",
    "\n",
    "folosim modelul pentru a face o predictie asupra instantelor de testare. Calculam eroarea comparand rezultatul obtinut cu label-ul real al instantei si ponderam aceasta eroare cu coeficientul de invatare si array-ul actual de greutati (weights). Cumulam aceaste eroari ponderate intr-un array cu acelasi shape ca si weights pentru a face update ulterior. Procedam asemanator si pentru bias."
   ],
   "id": "dcb225e6346b2d25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T07:23:26.573192Z",
     "start_time": "2024-10-21T07:23:26.557465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(x: np.array, y: np.array, weights: np.ndarray, bias: np.ndarray, alpha: float) -> (np.array, np.array):\n",
    "    delta = np.zeros_like(weights)\n",
    "    beta = np.zeros_like(bias)\n",
    "    for i in range(len(x)):\n",
    "        predicted_y = make_prediction(weights, bias, x[i])\n",
    "        error =  (y[i] - predicted_y)\n",
    "        delta = delta + alpha * np.dot(error.reshape(len(predicted_y), 1), np.atleast_2d(x[i]))\n",
    "        #delta = delta + alpha * error.reshape(len(predicted_y), 1) * np.tile(x[i], (len(weights), 1))\n",
    "        beta = beta + alpha * error\n",
    "    return delta, beta"
   ],
   "id": "508463885c0f537f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Simularea epocilor si initializarea parametrilor<br>\n",
    "Pentru un numar predefinit de epoci antrenam modelul pe training set. Intr-o epoca dam shuffle la setul de date, il impartim in batch-uri, calculam update-ul per batch si facem update la weights si bias de la batch la batch (un fel de mini batch training?).<br>\n",
    "<br>\n",
    "Q: De ce facem batches daca algoritmul nu poate fi paralelizat?"
   ],
   "id": "f772d86ae617b1a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T07:23:28.279923Z",
     "start_time": "2024-10-21T07:23:28.264303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_model(weights: np.array, bias: np.array, x: np.array, y: np.array, alpha: float, epochs: int, batch_size: int) -> None:\n",
    "    delta = np.zeros_like(weights)\n",
    "    beta = np.zeros_like(bias)\n",
    "    while epochs > 0:\n",
    "        perm = np.random.permutation(len(x))\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        for i in range(len(x) // batch_size):\n",
    "            delta1, beta1 = train(x[i * batch_size : (i + 1) * batch_size], y[i * batch_size : (i + 1) * batch_size], weights, bias, alpha)\n",
    "            # delta += delta1\n",
    "            # beta += beta1\n",
    "            weights += delta1\n",
    "            bias += beta1\n",
    "        epochs -= 1"
   ],
   "id": "607b96df9ab6c8b2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initializam weight-urile si bias-ul cu valori random pe care le normalizam.",
   "id": "c0c06833c3506bf0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T07:23:31.079193Z",
     "start_time": "2024-10-21T07:23:31.041286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def random_init_model() -> (np.array, np.array):\n",
    "    w = np.random.randn(10, 784)\n",
    "    b = np.random.randn(10)\n",
    "    \n",
    "    #normalize\n",
    "    w = (w - w.min()) / (w.max() - w.min())\n",
    "    b = (b - b.min()) / (b.max() - b.min())\n",
    "    \n",
    "    #scale\n",
    "    w = 2 * w - 1\n",
    "    b = 2 * b - 1\n",
    "    \n",
    "    return w, b"
   ],
   "id": "58e3e72c0c8581",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T07:23:33.461413Z",
     "start_time": "2024-10-21T07:23:33.445764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#se calculeaza acuratetea unui model dat fiind un set de date\n",
    "def assert_accuracy(weights: np.array, bias: np.array, test_x: np.array, test_y: np.array) -> float:\n",
    "    nr_successes = 0\n",
    "    for i in range(len(test_x)):\n",
    "        predicted_y = make_prediction(weights, bias, test_x[i])\n",
    "        if predicted_y.argmax() == test_y[i].argmax():\n",
    "            nr_successes += 1\n",
    "    return nr_successes / len(test_x)"
   ],
   "id": "9fd7a0be1f32403",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T21:26:25.089416Z",
     "start_time": "2024-10-19T21:26:24.843200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# #verificam acuratetea modelului initializat aleator\n",
    "# #ar trebui sa fie ~0.1\n",
    "# random_init_acc = assert_accuracy(w, b, test_X, test_Y)\n",
    "# print(random_init_acc)"
   ],
   "id": "9669863f7013611",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1253\n"
     ]
    }
   ],
   "execution_count": 497
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T21:32:57.645927Z",
     "start_time": "2024-10-19T21:31:58.089066Z"
    }
   },
   "cell_type": "code",
   "source": "# make_model(w, b, train_X, train_Y, alpha=0.001, epochs=20, batch_size=100)",
   "id": "cbab263bff47c05a",
   "outputs": [],
   "execution_count": 503
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T21:34:28.950710Z",
     "start_time": "2024-10-19T21:34:28.637103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# #verificam acuratetea modelului antrenat\n",
    "# model_acc = assert_accuracy(w, b, test_X, test_Y)\n",
    "# print(model_acc)"
   ],
   "id": "fb48cf8d01109f02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8956\n"
     ]
    }
   ],
   "execution_count": 504
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T07:24:41.137287Z",
     "start_time": "2024-10-21T07:24:41.121672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "params_list = [\n",
    "    {\"Epochs\": 1, \"Learning Rate\": 0.01},\n",
    "    {\"Epochs\": 1, \"Learning Rate\": 0.001},\n",
    "    {\"Epochs\": 10, \"Learning Rate\": 0.01},\n",
    "    {\"Epochs\": 10, \"Learning Rate\": 0.001},\n",
    "    {\"Epochs\": 20, \"Learning Rate\": 0.01},\n",
    "    {\"Epochs\": 20, \"Learning Rate\": 0.001},\n",
    "    {\"Epochs\": 100, \"Learning Rate\": 0.01},\n",
    "    {\"Epochs\": 100, \"Learning Rate\": 0.001},\n",
    "]"
   ],
   "id": "6db304139d703400",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T10:24:35.439123Z",
     "start_time": "2024-10-21T07:24:51.251808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "train_X, train_Y, test_X, test_Y = initialize_data()\n",
    "\n",
    "train_X = normalize_data(train_X)\n",
    "test_X = normalize_data(test_X)\n",
    "\n",
    "train_Y = one_hot_encode(train_Y, nr_classes = 10)\n",
    "test_Y = one_hot_encode(test_Y, nr_classes = 10)\n",
    "\n",
    "nr_tests_per_param = 10\n",
    "\n",
    "results = []\n",
    "for params in params_list:\n",
    "    for _ in range(nr_tests_per_param):\n",
    "        w, b = random_init_model()\n",
    "        \n",
    "        start = time.time()\n",
    "        make_model(w, b, train_X, train_Y, alpha=params[\"Learning Rate\"], epochs=params[\"Epochs\"], batch_size=100)\n",
    "        end = time.time()\n",
    "        \n",
    "        results.append({\"Epochs\": params[\"Epochs\"], \"Learning Rate\": params[\"Learning Rate\"], \"Training Time\": end - start, \"Accuracy\": assert_accuracy(w, b, test_X, test_Y)})"
   ],
   "id": "d6f35de509b2c7df",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T21:55:27.351032Z",
     "start_time": "2024-10-20T21:54:26.731549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w, b = random_init_model()\n",
    "make_model(w, b, train_X, train_Y, alpha=0.001, epochs=20, batch_size=100)"
   ],
   "id": "9fcf845d41baaa97",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T21:55:29.389969Z",
     "start_time": "2024-10-20T21:55:28.874925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_acc = assert_accuracy(w, b, test_X, test_Y)\n",
    "print(model_acc)"
   ],
   "id": "7ae84391ef54c969",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9211\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Rezultate",
   "id": "385d14b8bd4b2a59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T10:25:24.025919Z",
     "start_time": "2024-10-21T10:25:18.146380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"model_training_results.csv\", index=False)"
   ],
   "id": "30a8cdae04124173",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T10:25:38.759657Z",
     "start_time": "2024-10-21T10:25:38.619062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "averaged_results = df.groupby([\"Epochs\", \"Learning Rate\"]).agg({\n",
    "    \"Accuracy\": \"mean\",\n",
    "    \"Training Time\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "print(averaged_results)"
   ],
   "id": "1eff29a1d457f64c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epochs  Learning Rate  Accuracy  Training Time\n",
      "0       1          0.001   0.87724       3.084865\n",
      "1       1          0.010   0.90898       2.928307\n",
      "2      10          0.001   0.91762      47.750102\n",
      "3      10          0.010   0.91914      37.592853\n",
      "4      20          0.001   0.92084     108.425187\n",
      "5      20          0.010   0.92037     105.964745\n",
      "6     100          0.001   0.92533     284.712155\n",
      "7     100          0.010   0.91901     484.469546\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
