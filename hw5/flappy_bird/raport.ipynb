{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Deep Q Network plays Flappy Bird\n",
    "\n",
    "In this nn assignment we are supposed to train a neural net to play flappy bird. The library we used to simulate the game is flappy-bird-gymnasium (https://github.com/markub3327/flappy-bird-gymnasium).\n",
    "\n",
    "### Network architecture\n",
    "\n",
    "The Deep Q network architecture features 3 layers of convolution followed by 2 fully connected layers. The convolutional layers help extract important features from the input image, simplifying the objective of the fully connected layers, since they don't have to worry about exact pixel locations and can focus on more general aspects of the game state (like how close the bird is to a pipe)."
   ],
   "id": "265830814548dee1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class DQNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self._calculate_fc_input(input_shape), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions),\n",
    "        )\n",
    "\n",
    "    def _calculate_fc_input(self, input_shape):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)\n",
    "            output = self.conv(dummy_input)\n",
    "            return int(np.prod(output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(x.shape)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters\n",
    "\n",
    "* IMAGE_SIZE is the size to which we resize the image in the preprocessing step.\n",
    "* LEARNING_RATE is the learning rate of the dqn.<br>\n",
    "* GAMMA is the discount rate for the q learning algorithm.\n",
    "* BUFFER_SIZE is the maximum size of the replay buffer which is implemented using a deque.\n",
    "* BATCH_SIZE is the size of a batch that's used when the dqn is updated.\n",
    "* EPSILON_START is the starting value of epsilon in the implementation of epsilon greedy.\n",
    "* EPSILON_DECAY is the rate at which epsilon decays (epsilon *= EPSILON_DECAY).\n",
    "* EPSILON_MIN is the minimum value at which epsilon stops decaying.\n",
    "* TARGET_UPDATE_FREQ is the frequency at which the target_dqn is updated with the weights of the q_network.\n",
    "* NUM_EPISODES is the number of runs the algorithm simulates.\n",
    "\n"
   ],
   "id": "771183fdcf394cd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "IMAGE_SIZE = (63, 112)\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.99\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "EPSILON_START = 0.8\n",
    "EPSILON_DECAY = 0.99\n",
    "EPSILON_MIN = 0.01\n",
    "TARGET_UPDATE_FREQ = 100\n",
    "NUM_EPISODES = 1000"
   ],
   "id": "4129ce4e181c069a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Image preprocessing\n",
    "We trained our model on images rather than feature information (pipe positions, bird position etc.) since we wanted to experiment with a convolutional neural network architecture. Thus, we preprocessed the image in order to make the job easier for the dqn. <br>\n",
    "\n",
    "The methods we implemented are resizing, background removal, converting to grayscale and thresholding."
   ],
   "id": "48f11d8d6b3eb89c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess(image: np.ndarray, background: int = 200, threshold: int = 254):\n",
    "    image[image[:, :, 0] == background] = [255, 255, 255]\n",
    "    grayscale = 0.2989 * image[..., 0] + 0.5870 * image[..., 1] + 0.1140 * image[..., 2]\n",
    "    binary_image = (grayscale < threshold).astype(np.uint8)\n",
    "\n",
    "    return cv2.resize(binary_image, IMAGE_SIZE, interpolation=cv2.INTER_AREA)"
   ],
   "id": "e0761d6075755380"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The result of the preprocessing for a frame: <br>\n",
    "![Preprocessed frame](./images/preprocessed_frame.png \"Preprocessed frame\")"
   ],
   "id": "3e169f0d468edcab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Replay Buffer\n",
    "The replay buffer is a deque in which we memorize the description of the last BUFFER_SIZE states simulated by the algorithm.<br>\n",
    "The sample(self, batch_size) method is used to take a subset of batch_size states from the replay buffer in order to give them to the q_network.\n"
   ],
   "id": "2442c8d09f0e48b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ],
   "id": "c6d448a0e87af2f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The Q learning algorithm\n",
    "First we initialize the networks used and the replay buffer.<br>\n",
    "#### The used networks:\n",
    "* <b>The q_network</b> predicts the q_values of the current state should take given the preprocessed image of it.<br>\n",
    "* <b>The target_network</b> predicts the q_values for the next state given the preprocessed image of it.<br>\n",
    "We need to separate them because the q_network needs a fixed function to optimize. For TARGET_UPDATE_FREQ episodes, the target_network remains unchanged to help the q_network with consistent predictions of the q_values of the next state. After TARGET_UPDATE_FREQ episodes, the state of the q_network is copied into the state of the target_network.<br>\n",
    "If we were to use only one dqn, it would not converge since it would try to optimize a function predicted by itself, which changes everytime the network is updated.<br>\n",
    "#### Epsilon greedy\n",
    "The epsilon greedy method gives the model a chance to explore the search space by making a random action with probability of epsilon.<br>\n",
    "After every episode, the value of epsilon decays by EPSILON_DECAY.<br>\n",
    "#### Assessing the state of the game\n",
    "We use the env.step() method provided by the flappy-bird-gymnasium library in order to assess the immediate reward of our action.<br>\n",
    "We append the state, the action, the reward, the next_state, and whether the game has finished to the replay buffer.\n",
    "#### Training of the network\n",
    "1. First, we take a sample of size BATCH_SIZE from the replay buffer.\n",
    "2. We convert the components of each state tuple to tensors.\n",
    "3. We approximate the q values of each state in the batch using the q_network.\n",
    "4. We approximate the maximum q values for each next state using the target_network.\n",
    "5. We calculate the target values using the formula targets = rewards + (1 - dones) * GAMMA * max_next_q_values. We add the immediate reward of the state with the discounted maximum values for the next states. If the run ends on the next state, we don't add anythong to the reward, hence the (1 - dones).\n",
    "6. Compute the loss and backpropagate in order to update the q_network.\n",
    "7. Every TARGET_UPDATE_FREQ frames, we update the target_network's weights with the current network's weights.\n",
    "8. Decay epsilon by EPSILON_DECAY."
   ],
   "id": "37a2f8c56f40dfee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_dql(env):\n",
    "    # Initialize networks\n",
    "    input_shape = (1, *IMAGE_SIZE)  # Single grayscale channel\n",
    "    num_actions = env.action_space.n\n",
    "    q_network = DQNetwork(input_shape, num_actions).to(device)\n",
    "    target_network = DQNetwork(input_shape, num_actions).to(device)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=LEARNING_RATE)\n",
    "    replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "\n",
    "    epsilon = EPSILON_START\n",
    "    total_steps = 0\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        env.reset()\n",
    "        state = preprocess(env.render())\n",
    "        state = np.expand_dims(state, axis=0)  # Add channel dimension (1, 84, 84)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()  # Explore\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                    q_values = q_network(state_tensor)\n",
    "                    action = torch.argmax(q_values).item()  # Exploit\n",
    "\n",
    "            # Perform action in the environment\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = preprocess(env.render())\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "\n",
    "            # if episode >= 590:\n",
    "            #     print(f\"....{reward}\")\n",
    "\n",
    "            # Store transition in replay buffer\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Train the network\n",
    "            if replay_buffer.size() > BATCH_SIZE:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)\n",
    "                states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "                actions = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "                rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "                next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "                dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "\n",
    "                # Compute Q-values and targets\n",
    "                q_values = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                with torch.no_grad():\n",
    "                    max_next_q_values = target_network(next_states).max(1)[0]\n",
    "                    targets = rewards + (1 - dones) * GAMMA * max_next_q_values\n",
    "\n",
    "                # Compute loss and update Q-network\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_steps += 1\n",
    "\n",
    "            # Update target network periodically\n",
    "            if total_steps % TARGET_UPDATE_FREQ == 0:\n",
    "                target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon * EPSILON_DECAY, EPSILON_MIN)\n",
    "\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}, Score: {0}\")\n",
    "\n",
    "    env.close()\n",
    "    return q_network"
   ],
   "id": "60ede1ea51a1501f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "We ran the algorithm multiple times, changing the values of some hyperparameters.<br>\n",
    "| Episodes      | FC layers   | Average score      | Best score      |\n",
    "|---------------|---------------|---------------|---------------|\n",
    "| 1000 | 1 (512)  | 12.24  | 52  |\n",
    "| 1000  |  2 (512x256)  | 12.92  | 75  |\n",
    "| 10000  |  2 (512x256)  | 3.24  | 17  |\n"
   ],
   "id": "f200a893dab8ce0b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
