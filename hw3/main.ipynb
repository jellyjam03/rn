{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tema 3 - Multi Layer Perceptron",
   "id": "1ce9316206517ca4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-11T12:02:59.643279Z",
     "start_time": "2024-11-11T12:02:33.886837Z"
    }
   },
   "source": [
    "import math\n",
    "from venv import create\n",
    "\n",
    "import numpy as np\n",
    "from numpy.ma.core import reshape, argmax\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data',\n",
    "    transform=lambda x: np.array(x).flatten(),\n",
    "    download=True,\n",
    "    train=is_train)\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "    return mnist_data, mnist_labels"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initializarea si normalizarea datelor",
   "id": "614a650452144256"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T12:03:03.435269Z",
     "start_time": "2024-11-11T12:03:03.411790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_data(v: np.array) -> np.array:\n",
    "    return v / 256\n",
    "\n",
    "def one_hot_encode(v: np.array, nr_classes: int) -> np.array:\n",
    "    return np.array([np.array([int(i == label) for i in range(nr_classes)]) for label in v])\n",
    "\n",
    "def initialize_data() -> ():\n",
    "    train_x, train_y = download_mnist(True)\n",
    "    test_x, test_y = download_mnist(False)\n",
    "    \n",
    "    #convertim datele in np.array s\n",
    "    train_x = np.array(train_x)\n",
    "    train_y = np.array(train_y)\n",
    "    test_x = np.array(test_x)\n",
    "    test_y = np.array(test_y)\n",
    "    \n",
    "    train_x, test_x = normalize_data(train_x), normalize_data(test_x)\n",
    "    train_y, test_y = one_hot_encode(train_y, 10), one_hot_encode(test_y, 10)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y"
   ],
   "id": "361f105f122c706e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T12:03:41.977536Z",
     "start_time": "2024-11-11T12:03:05.650993Z"
    }
   },
   "cell_type": "code",
   "source": "train_x, train_y, test_x, test_y = initialize_data()",
   "id": "cc997bad4bd7c1df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initializarea si antreanarea modelului",
   "id": "b094df20a8b57c0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T12:03:45.409598Z",
     "start_time": "2024-11-11T12:03:45.386417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax(z: np.array) -> np.array:\n",
    "    z = z - np.max(z, axis=-1, keepdims=True)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n",
    "\n",
    "def sigmoid(z: np.array) -> np.array:\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z: np.array) -> np.array:\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "def relu_derivative(z: np.array) -> np.array:\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def cross_entropy_loss(y_true: np.array, y_pred: np.array) -> np.array:\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-9)) / y_true.shape[1]  # Added small value to avoid log(0)"
   ],
   "id": "c9c31a00aa4d0add",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Partea de stochastic gradient descent. Se observa partea de forward propagation in care trecem batch-ul prin retea, urmata de partea de backward propagation in care calculam derivatele functii de pierdere in raport cu valorile neuronilor fiecarui strat.<br>\n",
    "Pentru stratul de iesire avem $delta^2 = y^2 - labels$<br>\n",
    "Pentru stratul ascuns avem $delta^1 = delta^2 \\cdot (w^2)^T \\times ReLU'(z^1)$<br>\n",
    "La sfarsit actualizam ponderile si bias-urile inmultind cu learning rate-ul si impartind la batch size."
   ],
   "id": "a8476a1228edab00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T12:03:47.529328Z",
     "start_time": "2024-11-11T12:03:47.518216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "losses = []\n",
    "\n",
    "def train(instances: np.array, labels: np.array, hidden_layer: np.array, output_layer: np.array, alpha: float) -> None:\n",
    "    #print(hidden_layer[1][0])\n",
    "    #forward\n",
    "    instances = np.hstack((np.ones((instances.shape[0], 1)), instances))\n",
    "     \n",
    "    z1 = np.dot(instances, hidden_layer)\n",
    "    y1 = relu(z1)\n",
    "    \n",
    "    y1 = np.hstack((np.ones((y1.shape[0], 1)), y1))\n",
    "    z2 = np.dot(y1, output_layer)\n",
    "    y2 = softmax(z2) #np.array([softmax(row) for row in z2])\n",
    "    \n",
    "    losses.append(cross_entropy_loss(labels, y2))\n",
    "    \n",
    "    #backward\n",
    "    delta2 = y2 - labels\n",
    "    #delta1 = np.dot(delta2, output_layer[1:, :].T) * relu_derivative(z1) #[1:] because we don't take the bias into consideration when we compute dC/dy1\n",
    "    delta1 = np.dot(delta2, output_layer[1:, :].T) * relu_derivative(z1)\n",
    "    #print(instances)\n",
    "    #gradient descent\n",
    "    output_layer -= alpha * np.dot(y1.T, delta2) / instances.shape[0]\n",
    "    hidden_layer -= alpha * np.dot(instances.T, delta1) / instances.shape[0]\n",
    "    #print(instances.shape, delta1.shape)\n",
    "    "
   ],
   "id": "415c44d123617225",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T12:03:51.296577Z",
     "start_time": "2024-11-11T12:03:51.283347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_layer(in_size: int, out_size: int) -> np.array:\n",
    "    return np.random.randn(in_size, out_size) / np.sqrt(in_size)\n",
    "\n",
    "def create_model(epochs: int, alpha: float, batch_size: int, hidden_layer: np.array, output_layer: np.array, instances: np.array, labels:np.array) -> None:\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(instances.shape[0] // batch_size):\n",
    "            #print(np.max(hidden_layer[0]), np.max(output_layer[0]))\n",
    "            train(instances[i: i + batch_size], labels[i: i + batch_size], hidden_layer, output_layer, alpha)\n",
    "        #alpha *= 0.998"
   ],
   "id": "685a3bb079f81236",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T12:11:43.564866Z",
     "start_time": "2024-11-11T12:03:55.374275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hidden_layer = create_layer(785, 100) # 785 fiindca adaugam si bias-ul aici\n",
    "output_layer = create_layer(101, 10) # acelasi motiv pentru 101\n",
    "\n",
    "create_model(30, 0.001, 1, hidden_layer, output_layer, train_x, train_y)\n",
    "#print(hidden_layer[1][0])"
   ],
   "id": "12634438520df33e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Testarea modelului",
   "id": "e844e696eb46b3e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T12:12:04.500490Z",
     "start_time": "2024-11-11T12:12:04.486336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_prediction(instance: np.array, hidden_layer: np.array, output_layer: np.array) -> np.array:\n",
    "    instance = np.hstack((np.ones((instance.shape[0], 1)), instance))\n",
    "     \n",
    "    z1 = np.dot(instance, hidden_layer).T\n",
    "    y1 = relu(z1)\n",
    "    \n",
    "    y1 = np.vstack((np.ones((1, y1.shape[1])), y1))\n",
    "    z2 = np.dot(y1.T, output_layer)\n",
    "    y2 = softmax(z2) #np.array([softmax(row) for row in z2.T])\n",
    "    #print(z2, y2)\n",
    "    \n",
    "    return y2\n",
    "    \n",
    "def test_model(hidden_layer, output_layer, test_x: np.array, test_y: np.array) -> float:\n",
    "    good_guess = 0\n",
    "    for instance, label in zip(test_x, test_y):\n",
    "        #print(make_prediction(np.atleast_2d(instance), hidden_layer, output_layer))\n",
    "        if np.argmax(make_prediction(np.atleast_2d(instance), hidden_layer, output_layer)) == np.argmax(label):\n",
    "            good_guess += 1\n",
    "    return good_guess / len(test_y)"
   ],
   "id": "43e9ff4e228d81b9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T12:12:12.040819Z",
     "start_time": "2024-11-11T12:12:07.024224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(test_model(hidden_layer, output_layer, test_x, test_y))\n",
    "print(test_model(hidden_layer, output_layer, train_x, train_y))"
   ],
   "id": "a08fbf069949c4f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746\n",
      "0.9920666666666667\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T10:29:18.337418Z",
     "start_time": "2024-11-11T10:29:18.321157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "params_list = [\n",
    "    {\"Epochs\": 10, \"Learning Rate\": 0.01, \"Batch Size\": 100},\n",
    "    {\"Epochs\": 10, \"Learning Rate\": 0.001, \"Batch Size\": 100},\n",
    "    {\"Epochs\": 10, \"Learning Rate\": 0.01, \"Batch Size\": 10},\n",
    "    {\"Epochs\": 10, \"Learning Rate\": 0.001, \"Batch Size\": 10},\n",
    "    {\"Epochs\": 20, \"Learning Rate\": 0.01, \"Batch Size\": 100},\n",
    "    {\"Epochs\": 20, \"Learning Rate\": 0.001, \"Batch Size\": 100},\n",
    "    {\"Epochs\": 20, \"Learning Rate\": 0.01, \"Batch Size\": 10},\n",
    "    {\"Epochs\": 20, \"Learning Rate\": 0.001, \"Batch Size\": 10},\n",
    "    {\"Epochs\": 100, \"Learning Rate\": 0.01, \"Batch Size\": 100},\n",
    "    {\"Epochs\": 100, \"Learning Rate\": 0.001, \"Batch Size\": 100},\n",
    "    {\"Epochs\": 100, \"Learning Rate\": 0.01, \"Batch Size\": 10},\n",
    "    {\"Epochs\": 100, \"Learning Rate\": 0.001, \"Batch Size\": 10},\n",
    "]"
   ],
   "id": "1d561439abe4acf6",
   "outputs": [],
   "execution_count": 293
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T10:19:16.240415Z",
     "start_time": "2024-11-11T10:19:11.598174Z"
    }
   },
   "cell_type": "code",
   "source": "train_x, train_y, test_x, test_y = initialize_data()",
   "id": "97dbfba27f893b6a",
   "outputs": [],
   "execution_count": 287
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T10:47:23.201165Z",
     "start_time": "2024-11-11T10:29:20.146526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "nr_tests_per_param = 1\n",
    "\n",
    "results = []\n",
    "for params in params_list:\n",
    "    for _ in range(nr_tests_per_param):\n",
    "        print(params)\n",
    "        hidden_layer = create_layer(785, 100) # 785 fiindca adaugam si bias-ul aici\n",
    "        output_layer = create_layer(101, 10) # acelasi motiv pentru 101\n",
    "        create_model(10, 0.1, 100, hidden_layer, output_layer, train_x, train_y)\n",
    "        start = time.time()\n",
    "        create_model(params[\"Epochs\"], params[\"Learning Rate\"], params[\"Batch Size\"], hidden_layer, output_layer, train_x, train_y)\n",
    "        end = time.time()\n",
    "        \n",
    "        results.append({\"Epochs\": params[\"Epochs\"], \"Learning Rate\": params[\"Learning Rate\"], \"Batch Size\": params[\"Batch Size\"], \"Training Time\": end - start, \"Accuracy\": test_model(hidden_layer, output_layer, test_x, test_y)})"
   ],
   "id": "cc7b7f63ea103003",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epochs': 10, 'Learning Rate': 0.01, 'Batch Size': 100}\n",
      "{'Epochs': 10, 'Learning Rate': 0.001, 'Batch Size': 100}\n",
      "{'Epochs': 10, 'Learning Rate': 0.01, 'Batch Size': 10}\n",
      "{'Epochs': 10, 'Learning Rate': 0.001, 'Batch Size': 10}\n",
      "{'Epochs': 20, 'Learning Rate': 0.01, 'Batch Size': 100}\n",
      "{'Epochs': 20, 'Learning Rate': 0.001, 'Batch Size': 100}\n",
      "{'Epochs': 20, 'Learning Rate': 0.01, 'Batch Size': 10}\n",
      "{'Epochs': 20, 'Learning Rate': 0.001, 'Batch Size': 10}\n",
      "{'Epochs': 100, 'Learning Rate': 0.01, 'Batch Size': 100}\n",
      "{'Epochs': 100, 'Learning Rate': 0.001, 'Batch Size': 100}\n",
      "{'Epochs': 100, 'Learning Rate': 0.01, 'Batch Size': 10}\n",
      "{'Epochs': 100, 'Learning Rate': 0.001, 'Batch Size': 10}\n"
     ]
    }
   ],
   "execution_count": 294
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Rezultate",
   "id": "59a873f41d9630e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T10:47:43.218284Z",
     "start_time": "2024-11-11T10:47:43.187541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"model_training_results.csv\", index=False)"
   ],
   "id": "6a35ff8966f6baa9",
   "outputs": [],
   "execution_count": 295
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T10:47:45.492411Z",
     "start_time": "2024-11-11T10:47:45.363372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "averaged_results = df.groupby([\"Epochs\", \"Learning Rate\", \"Batch Size\"]).agg({\n",
    "    \"Accuracy\": \"mean\",\n",
    "    \"Training Time\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "print(averaged_results)"
   ],
   "id": "c45cc481c95eb1b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epochs  Learning Rate  Batch Size  Accuracy  Training Time\n",
      "0       10          0.001          10    0.9163      30.149229\n",
      "1       10          0.001         100    0.8632       7.468807\n",
      "2       10          0.010          10    0.9301      30.596278\n",
      "3       10          0.010         100    0.8655       6.951658\n",
      "4       20          0.001          10    0.9246      60.069152\n",
      "5       20          0.001         100    0.8637      14.971083\n",
      "6       20          0.010          10    0.9377      60.757613\n",
      "7       20          0.010         100    0.8616      15.142627\n",
      "8      100          0.001          10    0.9340     303.825330\n",
      "9      100          0.001         100    0.8623      75.375756\n",
      "10     100          0.010          10    0.9376     301.530252\n",
      "11     100          0.010         100    0.8615      77.504213\n"
     ]
    }
   ],
   "execution_count": 296
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interpretarea rezultatelor<br>\n",
    "Comparand cu rezultatele obrinute de perceptron la tema anterioara, pot deduce ca undeva am facut o greseala. Acuratetea modelului acestuia ar fi trebuit sa o depaseasca pe cea a perceptronului, el nemaifiind liniar. "
   ],
   "id": "c8ad6177c468fa6b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
